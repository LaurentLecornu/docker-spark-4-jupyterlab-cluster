{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02: Introduction to DataFrame (according to spark)\n",
    "\n",
    "\n",
    "## Quick guide to using DataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "python api that lists all the functions applicable to a dataframe.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html\n",
    "\n",
    "The aim of this tutorial is to start a spark session and manipulate dataframes by applying basic processing.\n",
    "\n",
    "1. Start a pyspark session\n",
    "\n",
    "2. Create a dataframe from an RDD or a list (don't forget to define a structure).\n",
    "\n",
    "3. Create a pandas dataframe from a dataframe (spark)\n",
    "\n",
    "4. Perform operations (compare with list processing)\n",
    "\n",
    "5. Grouping and joins\n",
    "\n",
    "6. Reading a csv\n",
    " \n",
    " https://notebooks.gesis.org/binder/jupyter/user/apache-spark-awl6064c/notebooks/python/docs/source/getting_started/quickstart_df.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python package list\n",
    "# !python --version\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start a spark session\n",
    "\n",
    "(we want to start a local session and give it a name).\n",
    "\n",
    "We'll use the SparkSession class from the [*pyspark.sql*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) sub-module .\n",
    "\n",
    "Call *sc* the *spark.sparkContext*.\n",
    "\n",
    "- *builder*: A class attribute having a Builder to construct SparkSession instances\n",
    "\n",
    "- Specify what is defined by [*SparkSession*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html) and *sparkContext*.\n",
    "\n",
    "- the master is 'local' (*master(\"local[\\*])* (it is possible to specify the number of cores *local[\\*]* or for example *local[4]*) \n",
    "\n",
    "- the application must be given a name *appName('name appli')*.\n",
    "\n",
    "- *getOrCreate()* : Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# link pyspark and spark\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m     13\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "\n",
    "# import pyspark and sparksession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# link pyspark and spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 08:22:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# define spark session\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName('TP02_dataframe').getOrCreate()\n",
    "# create sparkContext variable\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84298ff21fd8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TP02_dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=TP02_dataframe>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pypsark version == spark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A port (4040 by default) that you should definitely look at (link Spark UI)\n",
    "\n",
    "It allows you to follow the progress of a spark process (Spark UI).\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[*.parallelize()*](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html) : Distribute a local Python collection to form an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a dataframe\n",
    "\n",
    "### 2.1 Create a dataframe from rdd\n",
    "- create RDD from a list\n",
    "     - create list (list_1 and list_2)\n",
    "     - create rdd (rdd_l1 from list_1 and rdd_l2 from list_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:297\n",
      "[(1, 0), (1, 8), (1, 3), (1, 3), (1, 1), (1, 9), (1, 6), (1, 6), (1, 11), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# list\n",
    "list_1 = [(1, 0), (1, 8), (1, 3), (1, 3), (1, 1), (1, 9), (1, 6), (1, 6), (1, 11), (1, 2)]\n",
    "print(type(list_1))\n",
    "\n",
    "# Build RDD\n",
    "rdd_l1 = sc.parallelize(list_1, 2)\n",
    "print(rdd_l1)\n",
    "print(rdd_l1.collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:297\n",
      "[{'numero': 1, 'valeur': 0}, {'numero': 1, 'valeur': 2}]\n"
     ]
    }
   ],
   "source": [
    "list_2 = [{'numero': 1, 'valeur': 0},{'numero': 1, 'valeur': 2}]\n",
    "\n",
    "print(type(list_2))\n",
    "\n",
    "# Build RDD\n",
    "rdd_l2 = sc.parallelize(list_2, 2)\n",
    "print(rdd_l2)\n",
    "print(rdd_l2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create an RDD (resp. dl ) using the parallelize function of the sparkContext and the 2 previously created lists (list_1 and l1).\n",
    "- display the number of partitions using the getNumPartitions() function (from the RDD)`\n",
    "\n",
    "- Dataframe : \n",
    "\n",
    "[pysparl.sql.DataFrame]( https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html#)\n",
    "\n",
    "\n",
    "- Create a DataFrame \n",
    "\n",
    "[pyspark.sql.SparkSession.createDataFrame](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  1|  8|\n",
      "|  1|  3|\n",
      "|  1|  3|\n",
      "|  1|  1|\n",
      "|  1|  9|\n",
      "|  1|  6|\n",
      "|  1|  6|\n",
      "|  1| 11|\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "* Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [2]: [_1#0L, _2#1L]\n",
      "Arguments: [_1#0L, _2#1L], MapPartitionsRDD[6] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from a RDD \n",
    "df = spark.createDataFrame(list_1)\n",
    "\n",
    "# Display the dataframe (only a small one or a small part)\n",
    "df.show()\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| v2|value|\n",
      "+---+-----+\n",
      "|  1|    0|\n",
      "|  1|    8|\n",
      "|  1|    3|\n",
      "|  1|    3|\n",
      "|  1|    1|\n",
      "|  1|    9|\n",
      "|  1|    6|\n",
      "|  1|    6|\n",
      "|  1|   11|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's better with a column names\n",
    "df = spark.createDataFrame(rdd_l1,['v2', 'value'])\n",
    "\n",
    "# Display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v3: integer (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| v3|value|\n",
      "+---+-----+\n",
      "|  1|    0|\n",
      "|  1|    8|\n",
      "|  1|    3|\n",
      "|  1|    3|\n",
      "|  1|    1|\n",
      "|  1|    9|\n",
      "|  1|    6|\n",
      "|  1|    6|\n",
      "|  1|   11|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a schema (StructType)\n",
    "\n",
    "# dl1 has two columns\n",
    "schema =    StructType([\n",
    "        StructField(\"v3\", IntegerType(), True),\n",
    "        StructField(\"value\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "# use the previous rdd (dl1) to create dataframe df1\n",
    "df1 = spark.createDataFrame(rdd_l1,schema)\n",
    "\n",
    "# display schema\n",
    "df1.printSchema()\n",
    "\n",
    "# display result\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = true)\n",
      " |-- valeur: long (nullable = true)\n",
      "\n",
      "+------+------+\n",
      "|numero|valeur|\n",
      "+------+------+\n",
      "|     1|     0|\n",
      "|     1|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again with rdd_l2\n",
    "df = spark.createDataFrame(rdd_l2)\n",
    "\n",
    "# display schema\n",
    "df.printSchema()\n",
    "\n",
    "# display result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the elements number\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the elements numlber where value is > 5  in df (utiliser where et count)\n",
    "df.where('valeur > 5').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|numero|valeur|\n",
      "+------+------+\n",
      "|     1|     0|\n",
      "|     1|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>valeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numero  valeur\n",
       "0       1       0\n",
       "1       1       2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show()\n",
    "panda = df.toPandas()\n",
    "\n",
    "panda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a panda dataframe\n",
    "\n",
    "- Creating a spark dataframe from a panda dataframe\n",
    "\n",
    "- Display the dataframe and its schema\n",
    "\n",
    "We will use the list indicated in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.show()\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "panda_df\n",
      "   a    b        c           d                   e\n",
      "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
      "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
      "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [a#73L, b#74, c#75, d#76, e#77], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "a: bigint, b: double, c: string, d: date, e: timestamp\n",
      "LogicalRDD [a#73L, b#74, c#75, d#76, e#77], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [a#73L, b#74, c#75, d#76, e#77], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[a#73L,b#74,c#75,d#76,e#77]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"df.show()\")\n",
    "\n",
    "df.show()\n",
    "print(\"panda_df\")\n",
    "\n",
    "print(pandas_df)\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "-RECORD 2------------------\n",
      " a   | 3                   \n",
      " b   | 4.0                 \n",
      " c   | string3             \n",
      " d   | 2000-03-01          \n",
      " e   | 2000-01-03 12:00:00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display i columns (but it's possible in row)\n",
    "\n",
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataframe operation\n",
    "\n",
    "- select columns\n",
    "\n",
    "- add new columns by processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a dataframe (panda) consisting of 4 columns (1st column of type int, 2nd of type string, 3rd column a date, 4th column a float value)\n",
    "\n",
    "- name the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+-----+------------------+\n",
      "|summary|            indice| prenom|  nom|            valeur|\n",
      "+-------+------------------+-------+-----+------------------+\n",
      "|  count|                 4|      4|    4|                 4|\n",
      "|   mean|               2.5|   NULL| NULL|               2.5|\n",
      "| stddev|1.2909944487358056|   NULL| NULL|1.2909944487358056|\n",
      "|    min|                 1|jacques|  doe|               1.0|\n",
      "|    max|                 4| pierre|smith|               4.0|\n",
      "+-------+------------------+-------+-----+------------------+\n",
      "\n",
      "+------+-------+------+----------+------+\n",
      "|indice| prenom|   nom|      date|valeur|\n",
      "+------+-------+------+----------+------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|\n",
      "+------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'indice': [1, 2, 3, 4],\n",
    "    'prenom': ['john', 'jean', 'pierre', 'jacques'],\n",
    "    'nom': ['doe', 'dupond', 'smith', 'durand'],\n",
    "    'date': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1), date(2000, 4, 1)],\n",
    "    'valeur': [1., 2., 3., 4.]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add a 5th column indicating the type of data in the 2nd column\n",
    "\n",
    "- add a 6th column which is the result of an arithmetic operation on the 4th column\n",
    "  \n",
    "- display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+------+---------+\n",
      "|indice| prenom|   nom|      date|valeur|operation|\n",
      "+------+-------+------+----------+------+---------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|      4.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|      5.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|      6.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|      7.0|\n",
      "+------+-------+------+----------+------+---------+\n",
      "\n",
      "+------+-------+------+----------+------+--------------+\n",
      "|indice| prenom|   nom|      date|valeur|    nom+prenom|\n",
      "+------+-------+------+----------+------+--------------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|      john doe|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|   jean dupond|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|  pierre smith|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|jacques durand|\n",
      "+------+-------+------+----------+------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "indice: bigint, prenom: string, nom: string, date: date, valeur: double\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[indice#110L,prenom#111,nom#112,date#113,valeur#114]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"operation\",df.valeur +3).show()\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df.withColumn(\"nom+prenom\", concat_ws(\" \",\"prenom\",\"nom\")).show()\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply an operation on the column using a function\n",
    "\n",
    "(pandas functions can be applied)\n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+------+-----------------------------+\n",
      "|indice| prenom|   nom|      date|valeur|multiply_func(valeur, valeur)|\n",
      "+------+-------+------+----------+------+-----------------------------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|                          1.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|                          4.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|                          9.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|                         16.0|\n",
      "+------+-------+------+----------+------+-----------------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "* Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [5]: [indice#110L, prenom#111, nom#112, date#113, valeur#114]\n",
      "Arguments: [indice#110L, prenom#111, nom#112, date#113, valeur#114], MapPartitionsRDD[72] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "\n",
      "+------+-------+------+----------+------+-----------+\n",
      "|indice| prenom|   nom|      date|valeur|tripled_col|\n",
      "+------+-------+------+----------+------+-----------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|        3.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|        6.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|        9.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|       12.0|\n",
      "+------+-------+------+----------+------+-----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "indice: bigint, prenom: string, nom: string, date: date, valeur: double\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [indice#110L, prenom#111, nom#112, date#113, valeur#114], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[indice#110L,prenom#111,nom#112,date#113,valeur#114]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "@udf(\"float\") \n",
    "def tripled(num):\n",
    "  return 3*float(num)\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=FloatType())\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"*\",multiply(col(\"valeur\"), col(\"valeur\"))).show()\n",
    "df.explain(mode=\"formatted\")\n",
    "df.withColumn('tripled_col', tripled(df.valeur)).show()\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. The return of grouping, but applied to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following dataframe\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group by color and calculate average value and total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+-------+\n",
      "|color| fruit|avg(v1)|avg(v2)|\n",
      "+-----+------+-------+-------+\n",
      "|  red|banana|    4.0|   40.0|\n",
      "| blue|banana|    2.0|   20.0|\n",
      "|  red|carrot|    4.0|   40.0|\n",
      "| blue| grape|    4.0|   40.0|\n",
      "|black|carrot|    6.0|   60.0|\n",
      "|  red| grape|    8.0|   80.0|\n",
      "+-----+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(\"color\",\"fruit\").mean(\"v1\",\"v2\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge and add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+--------+---+---+\n",
      "|time    |id |v1 |time    |id |v2 |\n",
      "+--------+---+---+--------+---+---+\n",
      "|20000101|1  |1.0|20000101|1  |x  |\n",
      "|20000101|1  |1.0|20000101|2  |y  |\n",
      "|20000101|2  |2.0|20000101|1  |x  |\n",
      "|20000101|2  |2.0|20000101|2  |y  |\n",
      "+--------+---+---+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the two following dataframes:\n",
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "# use inner, left, right,...\n",
    "\n",
    "df1.join(df2,df1.time ==  df2.time,\"inner\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grouping to create a 4-column dataframe (time id v1 v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+---+\n",
      "|time    |id |v1 |id |v2 |\n",
      "+--------+---+---+---+---+\n",
      "|20000101|1  |1.0|1  |x  |\n",
      "|20000101|1  |1.0|2  |y  |\n",
      "|20000101|2  |2.0|1  |x  |\n",
      "|20000101|2  |2.0|2  |y  |\n",
      "+--------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the two time columns are identical\n",
    "df1.join(df2,[\"time\"],\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV file and create dataframe\n",
    "\n",
    "- read csv file and display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|Month;Passengers|\n",
      "|  2014-01;190111|\n",
      "|  2014-02;181712|\n",
      "|  2014-03;211813|\n",
      "|  2014-04;241231|\n",
      "|  2014-05;265480|\n",
      "|  2014-06;275565|\n",
      "|  2014-07;298342|\n",
      "|  2014-08;293137|\n",
      "|  2014-09;269074|\n",
      "|  2014-10;256367|\n",
      "|  2014-11;197014|\n",
      "|  2014-12;196291|\n",
      "|  2015-01;193029|\n",
      "|  2015-02;183588|\n",
      "|  2015-03;216866|\n",
      "|  2015-04;238951|\n",
      "|  2015-05;269396|\n",
      "|  2015-06;281930|\n",
      "|  2015-07;302005|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Passengers: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Month: date (nullable = true)\n",
      " |-- Passengers: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|     Month|Passengers|\n",
      "+----------+----------+\n",
      "|2014-01-01|    190111|\n",
      "|2014-02-01|    181712|\n",
      "|2014-03-01|    211813|\n",
      "|2014-04-01|    241231|\n",
      "|2014-05-01|    265480|\n",
      "|2014-06-01|    275565|\n",
      "|2014-07-01|    298342|\n",
      "|2014-08-01|    293137|\n",
      "|2014-09-01|    269074|\n",
      "|2014-10-01|    256367|\n",
      "|2014-11-01|    197014|\n",
      "|2014-12-01|    196291|\n",
      "|2015-01-01|    193029|\n",
      "|2015-02-01|    183588|\n",
      "|2015-03-01|    216866|\n",
      "|2015-04-01|    238951|\n",
      "|2015-05-01|    269396|\n",
      "|2015-06-01|    281930|\n",
      "|2015-07-01|    302005|\n",
      "|2015-08-01|    296076|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "root\n",
      " |-- Month: date (nullable = true)\n",
      " |-- Passengers: integer (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|     Month|Passengers|\n",
      "+----------+----------+\n",
      "|2014-01-01|    190111|\n",
      "|2014-02-01|    181712|\n",
      "|2014-03-01|    211813|\n",
      "|2014-04-01|    241231|\n",
      "|2014-05-01|    265480|\n",
      "|2014-06-01|    275565|\n",
      "|2014-07-01|    298342|\n",
      "|2014-08-01|    293137|\n",
      "|2014-09-01|    269074|\n",
      "|2014-10-01|    256367|\n",
      "|2014-11-01|    197014|\n",
      "|2014-12-01|    196291|\n",
      "|2015-01-01|    193029|\n",
      "|2015-02-01|    183588|\n",
      "|2015-03-01|    216866|\n",
      "|2015-04-01|    238951|\n",
      "|2015-05-01|    269396|\n",
      "|2015-06-01|    281930|\n",
      "|2015-07-01|    302005|\n",
      "|2015-08-01|    296076|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# read the file trafficaerien.csv\n",
    "\n",
    "dftext = spark.read.csv(\"data/trafficaerien.csv\")\n",
    "dftext.printSchema()\n",
    "\n",
    "dftext.show()\n",
    "\n",
    "dftext1 = spark.read.options(delimiter=';').option(\"header\",True).option(\"dateFormat\", \"yyyy-MM\").csv(\"data/trafficaerien.csv\")\n",
    "dftext1.printSchema()\n",
    "\n",
    "dftext1 = dftext1.withColumn('Month', to_date(dftext1['Month'], 'yyyy-MM'))\n",
    "\n",
    "dftext1.printSchema()\n",
    "\n",
    "dftext1.show()\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"Month\", DateType(), True),\n",
    "    StructField(\"Passengers\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "dftext2 = spark.read.options(delimiter=';').option(\"header\",True)\\\n",
    "   .option(\"dateFormat\", \"yyyy-MM\").option(\"inferSchema\", \"true\")\\\n",
    "   .csv(\"data/trafficaerien.csv\", schema = mySchema)\n",
    "\n",
    "\n",
    "dftext2.printSchema()\n",
    "\n",
    "dftext2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
